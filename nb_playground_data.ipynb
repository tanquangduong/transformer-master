{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# HuggingFace datasets and tokenizers\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from tdg_dataset import BilingualDataset\n",
    "\n",
    "from config import get_config\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 8,\n",
       " 'num_epochs': 20,\n",
       " 'lr': 0.0001,\n",
       " 'seq_len': 350,\n",
       " 'd_model': 512,\n",
       " 'datasource': 'opus_books',\n",
       " 'lang_src': 'en',\n",
       " 'lang_tgt': 'it',\n",
       " 'model_folder': 'weights',\n",
       " 'model_basename': 'tmodel_',\n",
       " 'preload': 'latest',\n",
       " 'tokenizer_file': 'tokenizer_{0}.json',\n",
       " 'experiment_name': 'runs/tmodel'}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw dataset\n",
    "ds_raw = load_dataset(f\"{config['datasource']}\", f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'translation'],\n",
       "    num_rows: 32332\n",
       "})"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32332"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_raw['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ds_raw['translation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Source: Project Gutenberg',\n",
       " 'it': 'Source: www.liberliber.it/Audiobook available here'}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_raw['translation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_all_sentences(ds, lang):\n",
    "    for item in ds:\n",
    "        yield item['translation'][lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_build_tokenizer(config, ds, lang):\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "    print(tokenizer_path)\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer_en.json\n",
      "tokenizer_it.json\n"
     ]
    }
   ],
   "source": [
    "# Build tokenizers\n",
    "tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n",
    "tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'translation'],\n",
       "    num_rows: 32332\n",
       "})"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_size = int(0.9 * len(ds_raw))\n",
    "val_ds_size = len(ds_raw) - train_ds_size\n",
    "train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29098"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3234"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 8,\n",
       " 'num_epochs': 20,\n",
       " 'lr': 0.0001,\n",
       " 'seq_len': 350,\n",
       " 'd_model': 512,\n",
       " 'datasource': 'opus_books',\n",
       " 'lang_src': 'en',\n",
       " 'lang_tgt': 'it',\n",
       " 'model_folder': 'weights',\n",
       " 'model_basename': 'tmodel_',\n",
       " 'preload': 'latest',\n",
       " 'tokenizer_file': 'tokenizer_{0}.json',\n",
       " 'experiment_name': 'runs/tmodel'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Project Gutenberg\n",
      "Encoding(num_tokens=4, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "['Source', ':', 'Project', '[UNK]']\n",
      "[5781, 38, 7699, 0]\n"
     ]
    }
   ],
   "source": [
    "for item in ds_raw:\n",
    "    print(item['translation'][config['lang_src']])\n",
    "    print(tokenizer_src.encode(item['translation'][config['lang_src']]))\n",
    "    print(tokenizer_src.encode(item['translation'][config['lang_src']]).tokens)\n",
    "    print(tokenizer_src.encode(item['translation'][config['lang_src']]).ids)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: www.liberliber.it/Audiobook available here\n",
      "Encoding(num_tokens=11, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "['Source', ':', '[UNK]', '.', 'liberliber', '.', 'it', '/', '[UNK]', 'available', 'here']\n",
      "[8161, 43, 0, 5, 19606, 5, 19516, 10657, 0, 13463, 14295]\n"
     ]
    }
   ],
   "source": [
    "for item in ds_raw:\n",
    "    print(item['translation'][config['lang_tgt']])\n",
    "    print(tokenizer_tgt.encode(item['translation'][config['lang_tgt']]))\n",
    "    print(tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).tokens)\n",
    "    print(tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of source sentence: 309\n",
      "Max length of target sentence: 274\n"
     ]
    }
   ],
   "source": [
    "# Find the maximum length of each sentence in the source and target sentence\n",
    "max_len_src = 0\n",
    "max_len_tgt = 0\n",
    "\n",
    "for item in ds_raw:\n",
    "    src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
    "    tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
    "    max_len_src = max(max_len_src, len(src_ids))\n",
    "    max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "\n",
    "print(f'Max length of source sentence: {max_len_src}')\n",
    "print(f'Max length of target sentence: {max_len_tgt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 8,\n",
       " 'num_epochs': 20,\n",
       " 'lr': 0.0001,\n",
       " 'seq_len': 350,\n",
       " 'd_model': 512,\n",
       " 'datasource': 'opus_books',\n",
       " 'lang_src': 'en',\n",
       " 'lang_tgt': 'it',\n",
       " 'model_folder': 'weights',\n",
       " 'model_basename': 'tmodel_',\n",
       " 'preload': 'latest',\n",
       " 'tokenizer_file': 'tokenizer_{0}.json',\n",
       " 'experiment_name': 'runs/tmodel'}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['seq_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tdg_dataset.BilingualDataset at 0x1fa52d84f10>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(\n",
    "            [tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_tgt.token_to_id(\"[SOS]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_src.token_to_id(\"[SOS]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_tgt.token_to_id(\"[EOS]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_src.token_to_id(\"[EOS]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_src.token_to_id(\"[PAD]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_src.token_to_id(\"[UNK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'translation': {'en': 'Source: Project Gutenberg',\n",
       "  'it': 'Source: www.liberliber.it/Audiobook available here'}}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '100',\n",
       " 'translation': {'en': 'Miss Abbot joined in-- \"And you ought not to think yourself on an equality with the Misses Reed and Master Reed, because Missis kindly allows you to be brought up with them.',\n",
       "  'it': 'La signorina Abbot soggiunse: — Spero che non vi crederete eguale alle signorine e al signor Reed, perché la signora è così buona da farvi educare insieme con loro.'}}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_raw[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 100\n",
    "seg_len = config['seq_len']\n",
    "src_target_pair = ds_raw[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '100',\n",
       " 'translation': {'en': 'Miss Abbot joined in-- \"And you ought not to think yourself on an equality with the Misses Reed and Master Reed, because Missis kindly allows you to be brought up with them.',\n",
       "  'it': 'La signorina Abbot soggiunse: — Spero che non vi crederete eguale alle signorine e al signor Reed, perché la signora è così buona da farvi educare insieme con loro.'}}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_target_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = src_target_pair[\"translation\"][config['lang_src']]\n",
    "tgt_text = src_target_pair[\"translation\"][config['lang_tgt']]\n",
    "\n",
    "# Transform the text into tokens\n",
    "enc_input_tokens = tokenizer_src.encode(src_text).ids\n",
    "dec_input_tokens = tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "# Add SOS, EOS and padding to each sentence\n",
    "enc_num_padding_token = (\n",
    "    seg_len - len(enc_input_tokens) - 2\n",
    ")  # We will add  <SOS> and <EOS> tokens\n",
    "dec_num_padding_token = (\n",
    "    seg_len - len(dec_input_tokens) - 1\n",
    ")  # We will add <EOS> toke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dec_input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La signorina Abbot soggiunse: — Spero che non vi crederete eguale alle signorine e al signor Reed, perché la signora è così buona da farvi educare insieme con loro.'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_token = torch.tensor(\n",
    "    [tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64\n",
    ")\n",
    "eos_token = torch.tensor(\n",
    "    [tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64\n",
    ")\n",
    "pad_token = torch.tensor(\n",
    "    [tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2]) tensor([3]) tensor([1])\n"
     ]
    }
   ],
   "source": [
    "print(sos_token, eos_token, pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[317,\n",
       " 2570,\n",
       " 1535,\n",
       " 13,\n",
       " 78,\n",
       " 35,\n",
       " 91,\n",
       " 24,\n",
       " 388,\n",
       " 21,\n",
       " 8,\n",
       " 153,\n",
       " 544,\n",
       " 36,\n",
       " 65,\n",
       " 8008,\n",
       " 22,\n",
       " 5,\n",
       " 5762,\n",
       " 698,\n",
       " 6,\n",
       " 3443,\n",
       " 698,\n",
       " 4,\n",
       " 173,\n",
       " 3324,\n",
       " 1536,\n",
       " 8843,\n",
       " 24,\n",
       " 8,\n",
       " 37,\n",
       " 291,\n",
       " 56,\n",
       " 22,\n",
       " 50,\n",
       " 7]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = torch.cat(\n",
    "        [\n",
    "            sos_token,\n",
    "            torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "            eos_token,\n",
    "            torch.tensor(\n",
    "                [pad_token] * enc_num_padding_token, dtype=torch.int64\n",
    "            ),\n",
    "        ],\n",
    "        dim=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,  317, 2570, 1535,   13,   78,   35,   91,   24,  388,   21,    8,\n",
       "         153,  544,   36,   65, 8008,   22,    5, 5762,  698,    6, 3443,  698,\n",
       "           4,  173, 3324, 1536, 8843,   24,    8,   37,  291,   56,   22,   50,\n",
       "           7,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = torch.cat(\n",
    "            [\n",
    "                sos_token,\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                torch.tensor(\n",
    "                    [pad_token] * dec_num_padding_token, dtype=torch.int64\n",
    "                ),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    2,    79,   357,  2496,   911,    43,     9,  2274,     8,    12,\n",
       "           64, 18158,  2365,   162,  2568,     6,    39,   175,   746,     4,\n",
       "           70,    11,   209,    27,    55,   400,    28,  2527,  7550,   347,\n",
       "           20,    69,     5,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                eos_token,\n",
    "                torch.tensor(\n",
    "                    [pad_token] * dec_num_padding_token, dtype=torch.int64\n",
    "                ),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   79,   357,  2496,   911,    43,     9,  2274,     8,    12,    64,\n",
       "        18158,  2365,   162,  2568,     6,    39,   175,   746,     4,    70,\n",
       "           11,   209,    27,    55,   400,    28,  2527,  7550,   347,    20,\n",
       "           69,     5,     3,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_mask(size):\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "            \"encoder_input\": encoder_input,  # Shape: (seq_len,)\n",
    "            \"decoder_input\": decoder_input,  # Shape: (seq_len,)\n",
    "            \"encoder_mask\": (encoder_input != pad_token)\n",
    "            .unsqueeze(0)\n",
    "            .unsqueeze(0)\n",
    "            .int(),  # Shape: (1, 1, seq_len)\n",
    "            \"decoder_mask\": (decoder_input != pad_token).unsqueeze(0).int()\n",
    "            & causal_mask(\n",
    "                decoder_input.shape[0]\n",
    "            ),  # Shape: (1, seq_len) &  (1, seq_len, seq_len)\n",
    "            \"label\": label,  # Shape: (seq_len,)\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([350])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([350])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(encoder_input != pad_token).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(encoder_input != pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(encoder_input != pad_token).unsqueeze(0).unsqueeze(0).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 350])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(encoder_input != pad_token).unsqueeze(0).unsqueeze(0).int().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(decoder_input != pad_token).unsqueeze(0).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 350])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(decoder_input != pad_token).unsqueeze(0).int().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False,  ..., False, False, False],\n",
       "         [ True,  True, False,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ..., False, False, False],\n",
       "         ...,\n",
       "         [ True,  True,  True,  ...,  True, False, False],\n",
       "         [ True,  True,  True,  ...,  True,  True, False],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True]]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_mask(decoder_input.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 350, 350])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['decoder_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output['decoder_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 0, 0, 0, 0],\n",
      "         [1, 1, 0, 0, 0],\n",
      "         [1, 1, 1, 0, 0],\n",
      "         [1, 1, 1, 0, 0],\n",
      "         [1, 1, 1, 0, 0]]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def causal_mask(size):\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "    return mask == 0\n",
    "\n",
    "# Let's say we have the following decoder input and pad token\n",
    "decoder_input = torch.tensor([1, 2, 3, 0, 0])  # 0 is the pad token\n",
    "pad_token = 0\n",
    "\n",
    "# Create a mask where True represents non-pad tokens\n",
    "non_pad_mask = (decoder_input != pad_token).unsqueeze(0).int()\n",
    "\n",
    "# Create a causal mask for the decoder input\n",
    "causal_mask = causal_mask(decoder_input.shape[0])\n",
    "\n",
    "# Combine the two masks\n",
    "combined_mask = non_pad_mask & causal_mask\n",
    "\n",
    "print(combined_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False, False, False],\n",
       "         [ True,  True, False, False, False],\n",
       "         [ True,  True,  True, False, False],\n",
       "         [ True,  True,  True,  True, False],\n",
       "         [ True,  True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 0, 0, 0],\n",
       "         [1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 0, 0]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['encoder_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 0,  ..., 0, 0, 0],\n",
       "         [1, 1, 0,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['decoder_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 350])\n"
     ]
    }
   ],
   "source": [
    "print(output['encoder_mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 350, 350])\n"
     ]
    }
   ],
   "source": [
    "print(output['decoder_mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
    "val_dataloader = DataLoader(val_ds, batch_size=config['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1fa50aa0a60>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1fa50aa33d0>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tdg_dataset.BilingualDataset at 0x1fa50a20f70>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
    "val_dataloader = DataLoader(val_ds, batch_size=config['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "{'encoder_input': tensor([[   2,  240, 3674,  ...,    1,    1,    1],\n",
      "        [   2,  337,   69,  ...,    1,    1,    1],\n",
      "        [   2, 5781,   38,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [   2,  117,  232,  ...,    1,    1,    1],\n",
      "        [   2, 4446,  447,  ...,    1,    1,    1],\n",
      "        [   2,   12,    9,  ...,    1,    1,    1]]), 'decoder_input': tensor([[    2,     0,   807,  ...,     1,     1,     1],\n",
      "        [    2,   241,    42,  ...,     1,     1,     1],\n",
      "        [    2,  8161,    43,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    2,     0,    45,  ...,     1,     1,     1],\n",
      "        [    2, 16685,   291,  ...,     1,     1,     1],\n",
      "        [    2,     9,   136,  ...,     1,     1,     1]]), 'encoder_mask': tensor([[[[1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1,  ..., 0, 0, 0]]]], dtype=torch.int32), 'decoder_mask': tensor([[[[1, 0, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1, 0, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0]]]], dtype=torch.int32), 'label': tensor([[    0,   807,   109,  ...,     1,     1,     1],\n",
      "        [  241,    42,    75,  ...,     1,     1,     1],\n",
      "        [ 8161,    43, 14298,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,    45,   357,  ...,     1,     1,     1],\n",
      "        [16685,   291,    38,  ...,     1,     1,     1],\n",
      "        [    9,   136,    12,  ...,     1,     1,     1]]), 'src_text': ['A pearl necklace I had given you lay untouched in its little casket; your trunks were left corded and locked as they had been prepared for the bridal tour.', 'As if there could be any direction and struggle in infinity!', 'Source: http://librosgratis.liblit.com/', 'Just then the Princess entered the room with the family doctor.', 'Landau, you see, was a commis [Shop-assistant.] in Paris and went to see a doctor.', 'You may tell Miss Smith that I forgot to make a memorandum of the darning needles, but she shall have some papers sent in next week; and she is not, on any account, to give out more than one at a time to each pupil: if they have more, they are apt to be careless and lose them.', 'Several times the couple tried to guess what was expected of them, and blundered each time, the priest prompting them in whispers.', \"'I don't see that it is a joke, that...' began Levin, but Koznyshev interrupted him.\"], 'tgt_text': ['Avevate lasciato anche il vezzo di perle che vi avevo dato, e i vostri bauli erano legati ancora come per il viaggio di nozze.', 'Come se ci potesse essere una qualche direzione e una lotta nell’infinito!', 'Source: http://lr.learnlangs.com/Translation: Maria Bianca LuporiniAudiobook available here', 'In quel momento la principessa entrò in salotto col medico curante.', 'Landau, vedete, era commis in un magazzino di Parigi ed era andato dal dottore.', \"Direte alla signorina Smith che ho dimenticato quelli da rammendare, ma la settimana prossima ne avrà qualche carta; guardi bene di non darne che uno per volta alle educande; potrebbero perderli e sarebbe un'occasione di disordine.\", 'Varie volte gli sposi cercarono di indovinare che cosa si dovesse fare, e ogni volta sbagliarono, e il prete li corresse sottovoce.', '— Io non vedo come questo sia uno scherzo, che... — voleva cominciare Levin, ma Sergej Ivanovic lo interruppe.']}\n",
      "Batch 2:\n",
      "{'encoder_input': tensor([[   2,   12,  617,  ...,    1,    1,    1],\n",
      "        [   2,   12,  185,  ...,    1,    1,    1],\n",
      "        [   2, 2265,   47,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [   2, 2497,   17,  ...,    1,    1,    1],\n",
      "        [   2, 2957,   44,  ...,    1,    1,    1],\n",
      "        [   2,   12,  237,  ...,    1,    1,    1]]), 'decoder_input': tensor([[    2,     9,  1273,  ...,     1,     1,     1],\n",
      "        [    2,     9,   668,  ...,     1,     1,     1],\n",
      "        [    2,   263,  1485,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    2, 10693,    10,  ...,     1,     1,     1],\n",
      "        [    2, 12804, 15102,  ...,     1,     1,     1],\n",
      "        [    2,     9,  2781,  ...,     1,     1,     1]]), 'encoder_mask': tensor([[[[1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1,  ..., 0, 0, 0]]]], dtype=torch.int32), 'decoder_mask': tensor([[[[1, 0, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1, 0, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0]]]], dtype=torch.int32), 'label': tensor([[    9,  1273,   535,  ...,     1,     1,     1],\n",
      "        [    9,   668,     4,  ...,     1,     1,     1],\n",
      "        [  263,  1485,  2251,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [10693,    10,    58,  ...,     1,     1,     1],\n",
      "        [12804, 15102, 14255,  ...,     1,     1,     1],\n",
      "        [    9,  2781,   567,  ...,     1,     1,     1]]), 'src_text': [\"'However, we will talk about all that later. What is this?\", \"'Yes, I noticed that poor Grinevich's nails interested you greatly,' said Oblonsky.\", 'Give one glance to my horrible life when you are gone.', \"'It is strange, but there are!\", \"'End it?\", \"Whether it was that he, Prince Oblonsky, a descendant of Rurik, was waiting two hours in a Jew's waiting-room, or that, for the first time in his life, he was departing from the example set by his ancestors of serving the State only and was entering on a new field, at any rate he felt uncomfortable.\", \"'--so long as I get somewhere,' Alice added as an explanation.\", \"'There is nobody better than you!...' he cried out in desperation through his tears, and seizing her by her shoulders he hugged her with all his might, his arms trembling with the effort.\"], 'tgt_text': ['— Del resto di questo parleremo poi.', '— Già, ho visto che le unghie del povero Grinevic ti interessavano molto — disse, ridendo, Stepan Arkad’ic.', 'Quando sarete partita, gettate uno sguardo sulla mia triste esistenza.', '— È strano, ma ce n’è.', '— Porre termine?', 'Ch’egli si sentisse a disagio perché lui, principe Oblonskij, discendente di Rjurik, aveva aspettato due ore nella sala d’aspetto di un giudeo, o perché, per la prima volta nella vita, non aveva seguito l’esempio degli antenati col servire il governo, e aveva voluto entrare in un’amministrazione nuova, certo è che si era sentito molto a disagio.', '—...purchè giunga in qualche parte, — riprese Alice come per spiegarsi meglio.', '— Più buono di te non c’è nessuno!... — egli gridò disperato attraverso le lacrime e, presala per le spalle, cominciò a stringerla a sé con tutta la forza, tremando nelle braccia per lo sforzo.']}\n"
     ]
    }
   ],
   "source": [
    "# Assuming train_dataloader is a DataLoader object\n",
    "for i, batch in enumerate(train_dataloader):\n",
    "    print(f\"Batch {i+1}:\")\n",
    "    print(batch)\n",
    "    # Only print the first 2 batches for brevity\n",
    "    if i >= 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['encoder_input', 'decoder_input', 'encoder_mask', 'decoder_mask', 'label', 'src_text', 'tgt_text'])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch['encoder_input'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([350])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['encoder_input'][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
